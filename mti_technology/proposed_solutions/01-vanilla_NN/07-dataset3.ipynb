{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52d13374",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from itertools import combinations, permutations\n",
    "from math import factorial\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7587ea97",
   "metadata": {},
   "source": [
    "## New Ideas\n",
    "- We would like to test on `X_new = np.random.randn(n_new_instances, 10)`. That is, for arrays of arbitrary floating-point numbers like `[0.45, 0.23, -0.08, -1.54, 1.12, -1.82, -1.25, 0.45, -0.39, -0.34]`, we would also like to see how our model's sorting ability is.\n",
    "  - Note that for our vanilla ANN model, the number `10`, i.e. the length of the array, is fixed and has to be fixed. If we want to adapt to all lengths, we might have to resort to RNN.\n",
    "  - This requires a similar but different dataset because in the past we have one-hotize our array elements and now we want the input to be the original, unprocessed array.\n",
    "  - As a consequence, we would probably need to add a preprocessing layer to the new model.\n",
    "- Implement these in `./07-dataset3.ipynb`\n",
    "- Just raw input like `[0.45, 0.23, -0.08, -1.54, 1.12, -1.82, -1.25, 0.45, -0.39, -0.34]`, or should we give the model the indices of each element as well? (To help increase the performance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9cd1eab8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9864090"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CEILING = 10**6\n",
    "FLOOR = -CEILING\n",
    "PADDER = 2*CEILING\n",
    "\n",
    "n_classes = 10\n",
    "max_length = 10\n",
    "n_instances = sum([reduce(lambda x, y: x*y, range(n_classes,n_classes-length,-1)) for length in range(2, max_length+1)])\n",
    "n_instances"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4294999",
   "metadata": {},
   "source": [
    "help(tf.concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "998f44ad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 4.], dtype=float32)>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([3., 4.])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec92713f",
   "metadata": {},
   "source": [
    "x = tf.zeros((10,), dtype=tf.float32)\n",
    "x[:2] = tf.constant([3., 4.])\n",
    "x"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6cf1901f",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------\n",
    "TypeError                                 Traceback (most recent call last)\n",
    "<ipython-input-8-07669b7fcccb> in <module>\n",
    "      1 x = tf.zeros((10,), dtype=tf.float32)\n",
    "----> 2 x[:2] = tf.constant([3., 4.])\n",
    "      3 x\n",
    "\n",
    "TypeError: 'tensorflow.python.framework.ops.EagerTensor' object does not support item assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eccf93b8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([3., 4., 0., 0., 0., 0., 0., 0., 0., 0.], dtype=float32)>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.concat([[3., 4.], tf.zeros((8,), dtype=tf.float32)], axis=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9b038f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4028235e+38"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.float32.max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35207fcf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PADDER < tf.float32.max"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8630fbb5",
   "metadata": {},
   "source": [
    "help(tf.fill)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9c5e4bb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=float32, numpy=array([9., 2.], dtype=float32)>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.constant([9, 2], dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94facd54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=\n",
       "array([9.e+00, 2.e+00, 2.e+06, 2.e+06, 2.e+06, 2.e+06, 2.e+06, 2.e+06,\n",
       "       2.e+06, 2.e+06], dtype=float32)>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [9, 2]\n",
    "x = tf.concat(\n",
    "        [tf.constant(p, dtype=tf.float32),\n",
    "         tf.fill((max_length - len(p),), float(PADDER))],\n",
    "        axis=0)\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0cc4e12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function tensorflow.python.ops.sort_ops.argsort(values, axis=-1, direction='ASCENDING', stable=False, name=None)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort"
   ]
  },
  {
   "cell_type": "raw",
   "id": "04c6aa03",
   "metadata": {},
   "source": [
    "help(tf.range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4c98d9e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 1., 2., 3., 4., 5., 6., 7., 8., 9.], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(0, 10, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c880fe28",
   "metadata": {},
   "source": [
    "y = tf.concat((tf.argsort(p),\n",
    "               tf.range(len(p), max_length, dtype=tf.float32)),\n",
    "              axis=0)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bba9a438",
   "metadata": {},
   "source": [
    "InvalidArgumentError: cannot compute ConcatV2 as input #1(zero-based) was expected to be a int32 tensor but is a float tensor [Op:ConcatV2] name: concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "80ab5cfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 0], dtype=int32)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(tf.constant(p, dtype=tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "77aeffa9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 0], dtype=int32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.argsort(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f13e73d",
   "metadata": {},
   "source": [
    "The error occurs because `tf.argsort()` always output a tensor of `dtype=tf.int32`"
   ]
  },
  {
   "cell_type": "raw",
   "id": "022eefb2",
   "metadata": {},
   "source": [
    "help(tf.argsort)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ab9aaaee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(8,), dtype=int32, numpy=array([2, 3, 4, 5, 6, 7, 8, 9], dtype=int32)>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.range(2, max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a6797ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(10,), dtype=int32, numpy=\n",
       "array([-100,  -99,    2,    3,    4,    5,    6,    7,    8,    9],\n",
       "      dtype=int32)>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.concat(([-100,-99], tf.range(2, max_length)), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "180e30c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_set_generator(test_proportion=0.2):\n",
    "    S = set(range(0, 9+1))\n",
    "    index_instance = 0\n",
    "    for length in range(2, max_length+1):    \n",
    "        for c in combinations(S, length):\n",
    "            for p in permutations(c):\n",
    "                if index_instance % 10 <= 10*test_proportion:\n",
    "                    index_instance += 1\n",
    "                    continue\n",
    "                x = tf.concat(\n",
    "                        [tf.constant(p, dtype=tf.float32),\n",
    "                         tf.fill((max_length - length,), float(PADDER))],\n",
    "                        axis=0)\n",
    "                #y = tf.concat((tf.cast(tf.argsort(p), dtype=tf.float32),\n",
    "                #               tf.range(length, max_length, dtype=tf.float32)))\n",
    "                y = tf.concat((tf.argsort(p), tf.range(length, max_length)), axis=0)\n",
    "                yield x, y\n",
    "                index_instance += 1\n",
    "\n",
    "def test_set_generator(test_proportion=0.2):\n",
    "    S = set(range(0, 9+1))\n",
    "    index_instance = 0\n",
    "    for length in range(2, max_length+1):    \n",
    "        for c in combinations(S, length):\n",
    "            for p in permutations(c):\n",
    "                if index_instance % 10 > 10*test_proportion:\n",
    "                    index_instance += 1\n",
    "                    continue\n",
    "                x = tf.concat(\n",
    "                        [tf.constant(p, dtype=tf.float32),\n",
    "                         tf.fill((max_length - length,), float(PADDER))],\n",
    "                        axis=0)\n",
    "                #y = tf.concat((tf.cast(tf.argsort(p), dtype=tf.float32),\n",
    "                #               tf.range(length, max_length, dtype=tf.float32)))\n",
    "                y = tf.concat((tf.argsort(p), tf.range(length, max_length)), axis=0)\n",
    "                yield x, y\n",
    "                index_instance += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d4e13c",
   "metadata": {},
   "source": [
    "It seems that we cannot combine the above two generator functions into a single one because the first arg of `tf.data.Dataset.from_generator()` has to be the generator itself, without parenthese."
   ]
  },
  {
   "cell_type": "raw",
   "id": "8c6815f0",
   "metadata": {},
   "source": [
    "def dataset_generator(is_train=True, test_proportion=0.2):\n",
    "    S = set(range(0, 9+1))\n",
    "    index_instance = 0\n",
    "    for length in range(2, max_length+1):    \n",
    "        for c in combinations(S, length):\n",
    "            for p in permutations(c):\n",
    "                if is_train:\n",
    "                    if index_instance % 10 <= 10*test_proportion:\n",
    "                        index_instance += 1\n",
    "                        continue\n",
    "                else:\n",
    "                    if index_instance % 10 > 10*test_proportion:\n",
    "                        index_instance += 1\n",
    "                        continue\n",
    "                x = tf.concat(\n",
    "                        [tf.constant(p, dtype=tf.float32),\n",
    "                         tf.fill((max_length - length,), float(PADDER))],\n",
    "                        axis=0)\n",
    "                #y = tf.concat((tf.cast(tf.argsort(p), dtype=tf.float32),\n",
    "                #               tf.range(length, max_length, dtype=tf.float32)))\n",
    "                y = tf.concat((tf.argsort(p), tf.range(length, max_length)), axis=0)\n",
    "                yield x, y\n",
    "                index_instance += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "43870168",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2 <= 10*0.2"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a4f7fe26",
   "metadata": {},
   "source": [
    "train_set = tf.data.Dataset.from_generator(\n",
    "    dataset_generator(is_train=True),\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "6db8f891",
   "metadata": {},
   "source": [
    "TypeError: `generator` must be callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0eb8496",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = tf.data.Dataset.from_generator(\n",
    "    train_set_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f521851a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "[2.e+00 0.e+00 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06]\n",
      "y =\n",
      "[1 0 2 3 4 5 6 7 8 9]\n",
      "x =\n",
      "[0.e+00 3.e+00 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06]\n",
      "y =\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "x =\n",
      "[3.e+00 0.e+00 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06]\n",
      "y =\n",
      "[1 0 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_set.take(3):\n",
    "    print(f\"x =\\n{x}\")\n",
    "    print(f\"y =\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "61c698ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = tf.data.Dataset.from_generator(\n",
    "    test_set_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.int32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1a936403",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x =\n",
      "[0.e+00 1.e+00 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06]\n",
      "y =\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "x =\n",
      "[1.e+00 0.e+00 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06]\n",
      "y =\n",
      "[1 0 2 3 4 5 6 7 8 9]\n",
      "x =\n",
      "[0.e+00 2.e+00 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06 2.e+06]\n",
      "y =\n",
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "for x, y in test_set.take(3):\n",
    "    print(f\"x =\\n{x}\")\n",
    "    print(f\"y =\\n{y}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b39107ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_shape = (max_length, max_length)\n",
    "model = keras.models.Sequential([\n",
    "    #keras.layers.Flatten(input_shape=max_length),\n",
    "    keras.layers.Dense(20, input_shape=(max_length,), activation=\"relu\"),\n",
    "    keras.layers.Dense(40, activation=\"relu\"),\n",
    "    #keras.layers.Dense(np.product(output_shape), activation=\"relu\"),\n",
    "    keras.layers.Dense(np.product(output_shape)),\n",
    "    keras.layers.Reshape(output_shape),\n",
    "    keras.layers.Softmax(axis=-1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "abd7255a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.shape =\n",
      "(10,)\n",
      "y.shape =\n",
      "(10,)\n",
      "x.shape =\n",
      "(10,)\n",
      "y.shape =\n",
      "(10,)\n",
      "x.shape =\n",
      "(10,)\n",
      "y.shape =\n",
      "(10,)\n"
     ]
    }
   ],
   "source": [
    "for x, y in train_set.take(3):\n",
    "    print(f\"x.shape =\\n{x.shape}\")\n",
    "    print(f\"y.shape =\\n{y.shape}\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4c249b6b",
   "metadata": {},
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dataset3_ANN.h5\")\n",
    "model.fit(train_set,\n",
    "          batch_size=32,\n",
    "          #validation_split=0.2,\n",
    "          callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e38178a",
   "metadata": {},
   "source": [
    "ValueError: Input 0 of layer sequential_1 is incompatible with the layer: expected axis -1 of input shape to have value 10 but received input with shape (10, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0622a947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = train_set.batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ec11e6c",
   "metadata": {},
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cbda866b",
   "metadata": {},
   "source": [
    "Epoch 1/7\n",
    "   4904/Unknown - 191s 39ms/step - loss: 14857.8114 - acc: 0.5291"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766aa0a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7\n",
      "215776/215776 [==============================] - 4829s 22ms/step - loss: 386.6764 - acc: 0.3355\n",
      "Epoch 2/7\n",
      " 12572/215776 [>.............................] - ETA: 1:15:09 - loss: 48786.6373 - acc: 0.4659"
     ]
    }
   ],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dataset3_ANN.h5\")\n",
    "\n",
    "model.fit(train_set,\n",
    "          batch_size=32,\n",
    "          #validation_split=0.2,\n",
    "          epochs=7,\n",
    "          callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1927b68",
   "metadata": {},
   "source": [
    "**(?)** Why `loss` decreases along with `acc` as time goes by?<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b89da79",
   "metadata": {},
   "source": [
    "## Bad Performance? Improvement.\n",
    "Looks like the model had difficulty elevating the accuracy. Maybe it does not know what `PADDER` means. Here are a few improving ideas:\n",
    "\n",
    "01. Assume all array elements are $\\ge 0\\,.$ And pick `PADDER = -1` and hopefully it will better understand what `PADDER` is.\n",
    "  - Add as the input layer of the model an activation layer to render all `-1`'s to `0`'s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c09147",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9047edb8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c671b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92abf456",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f11a58f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8edbffc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb580c5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "77ae9381",
   "metadata": {},
   "source": [
    "# `tf.data.Dataset`\n",
    "In previous notebooks, we have this code cell which is a memory hog (the `X`) and took long time to run.\n",
    "Here in this notebook, our objective is to construct the same dataset by using `tf` operations\n",
    "instead of `numpy` ones, hoping to reduce both memory usage and time (i.e. dataset construction time.)\n",
    "```python\n",
    "%%time\n",
    "S = set(range(0, 9+1))\n",
    "index_instance = 0\n",
    "for length in range(2, max_length+1):    \n",
    "    n_permutations = factorial(length)\n",
    "    for c in combinations(S, length):\n",
    "        for p in permutations(c):\n",
    "            X[index_instance, :length, :] = one_hot(np.array(p))\n",
    "            Y[index_instance, :] = np.concatenate((np.argsort(p), np.arange(length, max_length)))\n",
    "            index_instance += 1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02dbfeaa",
   "metadata": {},
   "source": [
    "## Workaround\n",
    "Maybe we should abandon the idea of using `tf.data.Dataset.from_tensor_slices(X)`, because that direction might always have to first allocate large memory.\n",
    "\n",
    "We start small and try to use `tf.data.Dataset`'s method to construct an equivalent datset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad0a003",
   "metadata": {},
   "source": [
    "**(?)** You've already seen in `ageron`'s homl2e that a dataset is able to contain tensors of diff shapes. Try to make an example yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fc94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = tf.range(2, max_length+1)\n",
    "dataset = tf.data.Dataset.from_tensor_slices(lengths)\n",
    "dataset = dataset.map(lambda x: tf.range(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e58345fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for tensor in dataset:\n",
    "    print(tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b370657c",
   "metadata": {},
   "source": [
    "**(?)** A big question that you haven't understood is: Should a `tf.data.Dataset` instance contain both `X` and `y`, i.e. data and labels, for supervised training? If so, how do we arrange `X` and `y`?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fea5a13",
   "metadata": {},
   "source": [
    "### First try: `tf.data.Dataset.from_generator()`\n",
    "As I imagine, we can keep the original code, keep the `for` loop, but instead of filling in each \"row\" of `X`, we make it a generator using the keyword `yield`. After implementing the generator using numpy, we pass the generator into `tf.data.Dataset.from_generator()` and we're done."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae4e971a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_generator():\n",
    "    S = set(range(0, 9+1))\n",
    "    index_instance = 0\n",
    "    for length in range(2, max_length+1):    \n",
    "        n_permutations = factorial(length)\n",
    "        for c in combinations(S, length):\n",
    "            for p in permutations(c):\n",
    "                x = np.zeros((max_length, n_classes), dtype=np.float32)\n",
    "                x[:length, :] = tf.one_hot(np.array(p),\n",
    "                                           depth=n_classes).numpy()\n",
    "                y = np.concatenate((np.argsort(p),\n",
    "                                    np.arange(length, max_length)))\n",
    "                yield x, y\n",
    "                index_instance += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a58c9b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    dataset_generator,\n",
    "    output_types=(tf.float32, tf.float32),\n",
    "    output_shapes=([max_length, n_classes], [max_length]),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec46a20c",
   "metadata": {},
   "source": [
    "dataset = tf.data.Dataset.from_generator(\n",
    "    dataset_generator,\n",
    "    output_signature=(\n",
    "        tf.TensorSpec(shape=(max_length, n_classes), dtype=tf.float32),\n",
    "        tf.TensorSpec(shape=(max_length,), dtype=tf.float32),\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5103c791",
   "metadata": {},
   "source": [
    "help(tf.data.Dataset.from_generator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95b2a9e7",
   "metadata": {},
   "source": [
    "**Rmk**. Had we forgotten to specify `output_shapes`, the following cells will still be able to run, up until\n",
    "`model.fit()`, which will generate the following error:\n",
    "```\n",
    "ValueError : as_list() is not defined on an unknown TensorShape\n",
    "```\n",
    "`model.fit()` is able to run once we specify both `output_types` and `output_shapes`.\n",
    "\n",
    "In the above, we have also provided (and disactivated) an equivalent cell using `output_signature` instead of the `(output_types, output_shapes)` pair, which is to be deprecated in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df6c7886",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for x, y in dataset.take(3):\n",
    "    print(f\"x =\\n{x}\")\n",
    "    print(f\"y =\\n{y}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d6869b",
   "metadata": {},
   "source": [
    "**Pros**\n",
    "\n",
    "01. We do not have to wait two to six minutes for `X` to be constructed any more\n",
    "02. Computers with little RAM can also run this code. Otherwise, they won't be able to even allocate enough memory for `X`.\n",
    "03. Compared to building a `tf.data.Dataset` completely from its methods, this `from_generator()` has the advantage of being a lot easier to implement. Actually, we almost only replaced the assignment of rows of `X` by `yield`\n",
    "\n",
    "**Cons**\n",
    "\n",
    "01. We must think of a way to split the dataset into Training/Validation/Test sets because we no longer have the entire `X` to apply `train_test_split` from `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aaae4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.batch(32, drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeeb1c6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for x, y in dataset.take(3):\n",
    "    print(f\"x.shape =\\n{x.shape}\")\n",
    "    print(f\"y.shape =\\n{y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf545ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://keras.io/api/layers/reshaping_layers/reshape/\n",
    "#https://keras.io/api/layers/activation_layers/softmax/\n",
    "input_shape = (max_length, n_classes)\n",
    "product_input_shape = np.product((max_length, n_classes))\n",
    "model = keras.models.Sequential([\n",
    "    keras.layers.Flatten(input_shape=input_shape),\n",
    "    keras.layers.Dense(product_input_shape, activation=\"relu\"),\n",
    "    #keras.layers.Dense(2*product_input_shape, activation=\"relu\"),\n",
    "    keras.layers.Dense(product_input_shape),\n",
    "    keras.layers.Reshape(input_shape),\n",
    "    keras.layers.Softmax(axis=-1),\n",
    "])\n",
    "\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=\"adam\",\n",
    "              metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "a770788d",
   "metadata": {},
   "source": [
    "help(model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fabcf1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"dataset3_ANN.h5\")\n",
    "model.fit(dataset,\n",
    "          batch_size=32,\n",
    "          callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4167d931",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa44e83d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745bba2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8055bdc0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb1bfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae0558b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4620c041",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cafdd1b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "470d4219",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a5ffdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "b3881711",
   "metadata": {},
   "source": [
    "def one_hot(array, depth=n_classes):\n",
    "    \"\"\"\n",
    "    array is an ndarray of shape (None,)\n",
    "    \"\"\"\n",
    "    #return tf.one_hot(array, depth=n_classes).numpy()\n",
    "    return np.eye(depth)[array, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b84ac45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "Y = np.empty((n_instances, max_length), dtype=np.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7613eda4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#X[...] = 0\n",
    "S = set(range(0, 9+1))\n",
    "index_instance = 0\n",
    "#for length in tqdm(range(2, max_length+1)):\n",
    "for length in range(2, max_length+1):    \n",
    "    n_permutations = factorial(length)\n",
    "    #n_combinations = n_instances // n_permutations\n",
    "    #for i, c in enumerate(combinations(S, length)):\n",
    "    for c in combinations(S, length):\n",
    "        #for j, p in enumerate(permutations(c)):\n",
    "        for p in permutations(c):\n",
    "            #print(f\"(index_instance/n_instances = {index_instance}/{n_instances})\", end=\"\\r\")\n",
    "            #print(f\"np.array(p) = {np.array(p)}\")\n",
    "            X[index_instance, :length, :] = one_hot(np.array(p))#[..., np.newaxis]\n",
    "            Y[index_instance, :] = np.concatenate((np.argsort(p), np.arange(length, max_length)))\n",
    "            index_instance += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21498e90",
   "metadata": {},
   "source": [
    "### Train/Validation/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae94af82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d50ba92",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2)\n",
    "X_train_val.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f19955be",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b1a5d6e",
   "metadata": {},
   "source": [
    "We might be able to use less neurons and still arrive at a similar performance. Running out of time, I had not tried to tune the model; instead, I had spent most of the time trying to implement more solutions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ffd4a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(\"vanilla_NN_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4e987d29",
   "metadata": {},
   "source": [
    "# add some callbacks before beginning training.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"vanilla_NN_model.h5\")\n",
    "\n",
    "model.fit(X_train_val,\n",
    "         Y_train_val,\n",
    "         #steps_per_epoch=60_000,\n",
    "         epochs=3,\n",
    "         validation_split=0.2,\n",
    "         verbose=True,\n",
    "         callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e864829a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(X_test, Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc19aae",
   "metadata": {},
   "source": [
    "## Evaluation on `X_test`\n",
    "We certainly would like to have performance measures like accuracy, precision/recall, etc. But we must first write some convenience functions to facilitate the operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe99ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sorter:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "\n",
    "    def lenlen(self, x):\n",
    "        somme = np.sum(x, axis=-1)\n",
    "        first_zero_index = -1\n",
    "        for i, s in enumerate(somme):\n",
    "            if s > 10**(-6):\n",
    "                first_zero_index = i\n",
    "        if first_zero_index == -1:\n",
    "            length = 10\n",
    "        else:\n",
    "            length = first_zero_index + 1\n",
    "        return length\n",
    "\n",
    "    def prettier(self, x, y):\n",
    "        \"\"\"\n",
    "        x.shape = (10,10)\n",
    "        \"\"\"\n",
    "        length = self.lenlen(x)\n",
    "        xx = np.argmax(x[:length], axis=-1)\n",
    "        sort_indices = y.astype(int)[:length]\n",
    "        yy = xx[sort_indices]\n",
    "        return xx, yy\n",
    "    \n",
    "    def evaluate(self, X, Y):\n",
    "        Y_pred = self.model.predict(X)  # of shape (n_instances, 10, 10)\n",
    "        Y = Y.astype(int)               # of shape (n_instances, 10)\n",
    "        m = X.shape[0]\n",
    "        n_correct = 0\n",
    "        for i, x in enumerate(X):\n",
    "            length = self.lenlen(x)\n",
    "            y_pred = Y_pred[i]\n",
    "            y_pred_sparse = np.argmax(y_pred, axis=-1)\n",
    "            n_correct += np.array_equal(Y[i], y_pred_sparse)\n",
    "        print(f\"acc = {n_correct/m}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c2c5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorter = Sorter(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e37bb68c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "sorter.evaluate(X_test, Y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
