{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ebe87d4c",
   "metadata": {},
   "source": [
    "# Vanilla Neural Network\n",
    "I think this question is less convenient to tackle using vanilla neural network than using sequential models like RNN, LSTM, etc. This is because vanilla NN normally deals with input with **fixed length**. To accomodate our need of different length from `2` to `10`, we would have to train `9` models, each with input shape `2, 3, 4, ...`.\n",
    "\n",
    "**Note**. Creating `10-2+1 = 9` NNs is not a bad thing. Their cooperated performance might also not mediocre. It is just that I find it more challenging/rewarding to try to find a devise-once-use-everywhere solution, so I ended up spending most of the time on finding such solutions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83219ad3",
   "metadata": {},
   "source": [
    "## Correction\n",
    "Actually, there exist ways to still use vanilla NN for this question. One such way is through padding. More precisely, let's take a few examples to illustrate our point:\n",
    "\n",
    "01. Input array `[0, 9, 7, 1, 2]`\n",
    "  - We may use integers $\\in {91, 92, 93, \\ldots, 98}$ to pad, whence the padded input array becomes\n",
    "  `[0, 9, 7, 1, 2, 91, 92, 93, 94, 95]`. We always pad until the padded array has length `10`.\n",
    "  - As for the corresponding output, I choose to return the permutation which make the padded input array sorted.\n",
    "  In this particular example,  that would be `[0, 3, 4, 2, 1, 5, 6, 7, 8, 9]`. Note how the last five indices\n",
    "  have not been altered at all in this permutation.\n",
    "02. Input array `[9, 0]`\n",
    "  - The padded input array would be `[9, 0, 91, 92, 93, 94, 95, 96, 97, 98]`.\n",
    "  - The output would be `[1, 0, 2, 3, 4, 5, 6, 7, 8, 9]`.\n",
    "\n",
    "This looks a little involved and artificial; nevertheless, it also brings convenience\n",
    "\n",
    "- The input and output are now both of fixed shape\n",
    "- The fact of being of fixed shape makes creating the dataset a lot more easier, (which in turn makes splitting it into Train/Validation/Test sets easier).\n",
    "\n",
    "### Correction inside correction\n",
    "We have said that we wanted to use `91..98` as padders, but, sorry, because normally we would use one-hot encoding during the dataset preparation for `X`. It's clueless how these `91..98` should be mapped. So, let's just forget the `91..98` idea, just **pad with zero vectors**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5454f34e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[91, 92, 93, 94, 95, 96, 97, 98]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padders = list(range(91, 98+1))\n",
    "padders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24b10df9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np\n",
    "from functools import reduce\n",
    "from itertools import combinations, permutations\n",
    "from math import factorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "542bde8a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.59868765, 0.40131232],\n",
       "       [0.4750208 , 0.5249792 ]], dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.softmax([[0.4, 0], [0.1, 0.2]]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "28042be0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 3,  5],\n",
       "        [10, 12]]),\n",
       " array([[ 0,  1,  2,  3,  4,  5,  6],\n",
       "        [ 7,  8,  9, 10, 11, 12, 13]]))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.arange(2*7).reshape((2,7))\n",
    "A[:, [3,5]], A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41b48188",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(tf.math.softmax([[0.4, 0, 0.1], [0.1, 0.2, 0.1]]).numpy(), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6755f0c",
   "metadata": {},
   "source": [
    "The following `X` will be our dataset (including training/validation/test sets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7ecd58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9864090"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_classes = 10\n",
    "max_length = 10\n",
    "n_instances = sum([reduce(lambda x, y: x*y, range(n_classes,n_classes-length,-1)) for length in range(2, max_length+1)])\n",
    "n_instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3e3b00ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.zeros((n_instances, max_length, n_classes), dtype=np.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60595c64",
   "metadata": {},
   "source": [
    "I have said in `README.md` that CNN is of little use here because we are not dealing with images. However, the shape of `X` does look like a single-channel image. Still, using CNN to extract local features makes little sense, so we will probably stick to our plan -- Maybe the first layer of our vanilla NN would be a `keras.layers.Flatten` and followed by a few fully connected layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c14aacd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 3, 1, 0])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argsort([9,5,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4895f973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 3, 5, 9])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A = np.array([9,5,0,3])\n",
    "A[np.argsort(A)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0826a909",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(array, depth=n_classes):\n",
    "    \"\"\"\n",
    "    array is an ndarray of shape (None,)\n",
    "    \"\"\"\n",
    "    return np.eye(depth)[array, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9667b693",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f9c134a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 10), dtype=float32, numpy=\n",
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.one_hot(A, depth=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "41b225bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels\n",
    "Y = np.empty((n_instances, max_length), dtype=np.float32)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4514c362",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(([1,2,3], [4,5,6]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5183a74a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 0, 3, 4, 2, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = (4,3,9,7,8)\n",
    "length = len(p)\n",
    "np.concatenate((np.argsort(p), np.arange(length, max_length)))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "7ead4953",
   "metadata": {},
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "raw",
   "id": "83c18674",
   "metadata": {},
   "source": [
    "#%%time\n",
    "S = set(range(0, 9+1))\n",
    "index_instance = 0\n",
    "#for length in tqdm(range(2, max_length+1)):\n",
    "for length in range(2, max_length+1):    \n",
    "    n_permutations = factorial(length)\n",
    "    #n_combinations = n_instances // n_permutations\n",
    "    #for i, c in enumerate(combinations(S, length)):\n",
    "    for c in combinations(S, length):\n",
    "        #for j, p in enumerate(permutations(c)):\n",
    "        for p in permutations(c):\n",
    "            #print(f\"(index_instance/n_instances = {index_instance}/{n_instances})\", end=\"\\r\")\n",
    "            #print(f\"np.array(p) = {np.array(p)}\")\n",
    "            X[index_instance, :length, :] = one_hot(np.array(p))#[..., np.newaxis]\n",
    "            Y[index_instance, :] = np.concatenate((np.argsort(p), np.arange(length, max_length)))\n",
    "            #print(f\"\"\"\n",
    "            #(index_instance/n_instances = {index_instance}/{n_instances})\n",
    "            #x = {one_hot(np.array(p))}\n",
    "            #y = {np.concatenate((np.argsort(p), np.arange(length, max_length)))}\n",
    "            #\"\"\", end=\"\\r\")\n",
    "            index_instance += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d50905",
   "metadata": {},
   "source": [
    "**(?)** Improvement. The above construction of `X, Y` takes a little long (6min on Thinkpad X200), can consider using concurrent programming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c00bdd4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9864090, 10, 10), dtype('float32'))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[...] = 0\n",
    "X.shape, X.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3ccdf8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent.futures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d91172d0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing length=2 Processing length=5 Processing length=4 Processing length=3 \n",
      "\n",
      "\n",
      "index_instance = 810index_instance = 5850\n",
      "index_instance = 0index_instance = 90\n",
      "\n",
      "\n",
      "\n",
      "x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "y = [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]y = [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "\n",
      "y = [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "y = [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]x = [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "\n",
      "\n",
      "x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "y = [1. 0. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "y = [0. 2. 1. 3. 4. 5. 6. 7. 8. 9.]y = [0. 1. 2. 4. 3. 5. 6. 7. 8. 9.]\n",
      "\n",
      "\n",
      "x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]x = [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]y = [0. 1. 3. 2. 4. 5. 6. 7. 8. 9.]\n",
      "\n",
      "\n",
      "\n",
      "y = [0. 1. 2. 3. 4. 5. 6. 7. 8. 9.]x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]]y = [0. 1. 3. 2. 4. 5. 6. 7. 8. 9.]y = [1. 0. 2. 3. 4. 5. 6. 7. 8. 9.]\n",
      "\n",
      "y = [0. 2. 1. 3. 4. 5. 6. 7. 8. 9.]\n",
      "x = [[0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]]x = [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "x = [[1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#X[...] = 0\n",
    "lengths = list(range(2, max_length+1))\n",
    "S = set(range(0, 9+1))\n",
    "\n",
    "def fill_X(length, X=X, Y=Y):\n",
    "    print(f\"Processing length={length} \")\n",
    "    index_instance = sum([reduce(lambda x, y: x*y, range(10,10-l,-1)) for l in range(2, length)])\n",
    "    print(f\"index_instance = {index_instance}\")\n",
    "    for c in combinations(S, length):\n",
    "        for p in permutations(c):\n",
    "            #print(f\"(index_instance/n_instances = {index_instance}/{n_instances})\", end=\"\\r\")\n",
    "            #print(f\"np.array(p) = {np.array(p)}\")\n",
    "            X[index_instance, :length, :] = one_hot(np.array(p))#[..., np.newaxis]\n",
    "            Y[index_instance, :] = np.concatenate((np.argsort(p), np.arange(length, max_length)))\n",
    "            print(f\"x = {one_hot(np.array(p))}\")\n",
    "            print(f\"y = {Y[index_instance, :]}\")\n",
    "            index_instance += 1\n",
    "    print(f\"Finished length={length} \")\n",
    "\n",
    "with concurrent.futures.ProcessPoolExecutor() as executor:\n",
    "    executor.map(fill_X, lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0ad36797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[index] =\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Y[index] = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = half = n_instances // 2\n",
    "print(f\"X[index] =\\n{X[index]}\")\n",
    "print(f\"Y[index] = {Y[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7cf8d971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[index] =\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Y[index] = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = -1\n",
    "print(f\"X[index] =\\n{X[index]}\")\n",
    "print(f\"Y[index] = {Y[index]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6fd49c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X[index] =\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Y[index] = [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "index = 1\n",
    "print(f\"X[index] =\\n{X[index]}\")\n",
    "print(f\"Y[index] = {Y[index]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b48ad9",
   "metadata": {},
   "source": [
    "This multi-threaded code has bug -- I think it's because `X` and `Y` are not really getting updated. Besides, the length `10` task is heaviest, way heavier than length `<=7`, so rearrange the workload."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c764c9f9",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e02e9c6c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3ae129",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
