{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "87655094",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1395b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "635f3671",
   "metadata": {},
   "outputs": [],
   "source": [
    "S = set(range(0, 9+1))\n",
    "n_classes = 10  # i.e. the array elements are allowed to be 0..9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e71aea1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9864090"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum([reduce(lambda x, y: x*y, range(10,10-length,-1))\n",
    "     for length in range(2, 10+1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7b0f7362",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import combinations, permutations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fdf3678e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(array, depth=n_classes):\n",
    "    \"\"\"\n",
    "    array is an ndarray/list of shape (None,)\n",
    "    \"\"\"\n",
    "    return np.eye(depth)[array, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b0515438",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def train_generator(batch_size=10):\n",
    "    shuffled_lengths = list(range(2, 10+1))\n",
    "    np.random.shuffle(shuffled_lengths)\n",
    "    for length in shuffled_lengths:\n",
    "        n_instances = reduce(lambda x, y: x*y, range(10,10-length,-1))\n",
    "        X = np.empty((n_instances, length, 1))\n",
    "        Y = np.empty((n_instances, length, n_classes))\n",
    "        n_permutations = math.factorial(length)\n",
    "        #n_combinations = n_instances // n_permutations\n",
    "        for i, c in enumerate(combinations(S, length)):\n",
    "            c_sorted = np.array(sorted(c))  # shape (length,)\n",
    "            c_onehot = one_hot(c_sorted)    # shape (length, n_classes)\n",
    "            Y[i*n_permutations : i*n_permutations + n_permutations, ...] = c_onehot\n",
    "            for j, p in enumerate(permutations(c)):\n",
    "                X[i*n_permutations : i*n_permutations + j, ...] = np.array(p)[..., np.newaxis]\n",
    "        # Throw one batch after another to the model\n",
    "        for k in range(n_instances // batch_size):\n",
    "            yield X[k*batch_size: (k+1)*batch_size].astype(np.float32), Y[k*batch_size: (k+1)*batch_size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dddff4fb",
   "metadata": {},
   "source": [
    "## Seq-to-Seq Model\n",
    "This is the seq-to-seq model in which the output sequence's length equals the input sequence length. We had better used **bidirectional RNNs**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ae51f19b",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_LSTM_model = keras.models.Sequential([\n",
    "    keras.layers.Bidirectional(keras.layers.LSTM(10,\n",
    "        return_sequences=True, input_shape=[None, 1], dropout=0.2)),\n",
    "    keras.layers.LSTM(10, return_sequences=True, dropout=0.2),\n",
    "    #keras.layers.Bidirectional(keras.layers.LSTM(10,\n",
    "    #    return_sequences=True, dropout=0.2)),\n",
    "    keras.layers.TimeDistributed(keras.layers.Dense(n_classes, activation=\"softmax\")),\n",
    "])\n",
    "\n",
    "#seq2seq_LSTM_model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\")\n",
    "# sparse_categorical_crossentropy would fail. Cf.\n",
    "# https://stackoverflow.com/questions/49161174/tensorflow-logits-and-labels-must-have-the-same-first-dimension\n",
    "seq2seq_LSTM_model.compile(loss=\"categorical_crossentropy\",\n",
    "                           optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139103cc",
   "metadata": {},
   "source": [
    "Before we jump into the potentially time-consuming training (via the `fit()` method), we can ask our model make a few predictions first to see if we implemented everything right.\n",
    "01. With `dropout=0.2` in the first layer and predicting on `np.array([0,9,8,3]` produced\n",
    "    ```\n",
    "    TypeError: Input 'b' of 'MatMul' Op has type float32 that does not match type int64 of argument 'a'.\n",
    "    ```\n",
    "    And if we execute the cell again, we will get\n",
    "    ```\n",
    "    TypeError: 'NoneType' object is not callable\n",
    "    ```\n",
    "    Quite weird behaviour.\n",
    "    - Actually, not `dropout=0.2`'s fault, the same error remains when we erase that input arg.\n",
    "02. Once the input has been corrected to an ndarray of `dtype` equal to `float` and the ndarray shape to `n_batches, length (aka n_timesteps), 1`, prediction works no problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dbd616ad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.09923745, 0.09971456, 0.09588277, 0.10505932, 0.10148352,\n",
       "         0.09433642, 0.09869379, 0.10522118, 0.10085614, 0.09951484],\n",
       "        [0.09802853, 0.0985541 , 0.09458166, 0.10814386, 0.1006169 ,\n",
       "         0.0899155 , 0.10120741, 0.1100034 , 0.10040047, 0.09854813],\n",
       "        [0.0975099 , 0.09791692, 0.09470064, 0.10864436, 0.09847064,\n",
       "         0.08752695, 0.10588092, 0.11358254, 0.09933651, 0.09643061],\n",
       "        [0.0977787 , 0.09857328, 0.09593067, 0.10680202, 0.09628848,\n",
       "         0.08824711, 0.11002257, 0.11381767, 0.09820938, 0.0943301 ]]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = seq2seq_LSTM_model.predict(np.array([0.,9,8,3]).reshape((1,4,1)), batch_size=1)\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a332d16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 4, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a535ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 7, 7, 7])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(output[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9d09f562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 4, 10)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 4\n",
    "bs = 7\n",
    "X_new = np.empty((bs, l, 1))\n",
    "for i in range(X_new.shape[0]):\n",
    "    X_new[i] = np.random.choice(range(0,9+1), l, replace=False).astype(np.float32).reshape((-1, 1))\n",
    "output = seq2seq_LSTM_model.predict(X_new)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ebf47b",
   "metadata": {},
   "source": [
    "Let's try the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "97fe2494",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "df1dab69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method fit in module tensorflow.python.keras.engine.training:\n",
      "\n",
      "fit(x=None, y=None, batch_size=None, epochs=1, verbose=1, callbacks=None, validation_split=0.0, validation_data=None, shuffle=True, class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None, validation_steps=None, validation_batch_size=None, validation_freq=1, max_queue_size=10, workers=1, use_multiprocessing=False) method of tensorflow.python.keras.engine.sequential.Sequential instance\n",
      "    Trains the model for a fixed number of epochs (iterations on a dataset).\n",
      "    \n",
      "    Arguments:\n",
      "        x: Input data. It could be:\n",
      "          - A Numpy array (or array-like), or a list of arrays\n",
      "            (in case the model has multiple inputs).\n",
      "          - A TensorFlow tensor, or a list of tensors\n",
      "            (in case the model has multiple inputs).\n",
      "          - A dict mapping input names to the corresponding array/tensors,\n",
      "            if the model has named inputs.\n",
      "          - A `tf.data` dataset. Should return a tuple\n",
      "            of either `(inputs, targets)` or\n",
      "            `(inputs, targets, sample_weights)`.\n",
      "          - A generator or `keras.utils.Sequence` returning `(inputs, targets)`\n",
      "            or `(inputs, targets, sample_weights)`.\n",
      "          A more detailed description of unpacking behavior for iterator types\n",
      "          (Dataset, generator, Sequence) is given below.\n",
      "        y: Target data. Like the input data `x`,\n",
      "          it could be either Numpy array(s) or TensorFlow tensor(s).\n",
      "          It should be consistent with `x` (you cannot have Numpy inputs and\n",
      "          tensor targets, or inversely). If `x` is a dataset, generator,\n",
      "          or `keras.utils.Sequence` instance, `y` should\n",
      "          not be specified (since targets will be obtained from `x`).\n",
      "        batch_size: Integer or `None`.\n",
      "            Number of samples per gradient update.\n",
      "            If unspecified, `batch_size` will default to 32.\n",
      "            Do not specify the `batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        epochs: Integer. Number of epochs to train the model.\n",
      "            An epoch is an iteration over the entire `x` and `y`\n",
      "            data provided.\n",
      "            Note that in conjunction with `initial_epoch`,\n",
      "            `epochs` is to be understood as \"final epoch\".\n",
      "            The model is not trained for a number of iterations\n",
      "            given by `epochs`, but merely until the epoch\n",
      "            of index `epochs` is reached.\n",
      "        verbose: 0, 1, or 2. Verbosity mode.\n",
      "            0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
      "            Note that the progress bar is not particularly useful when\n",
      "            logged to a file, so verbose=2 is recommended when not running\n",
      "            interactively (eg, in a production environment).\n",
      "        callbacks: List of `keras.callbacks.Callback` instances.\n",
      "            List of callbacks to apply during training.\n",
      "            See `tf.keras.callbacks`. Note `tf.keras.callbacks.ProgbarLogger`\n",
      "            and `tf.keras.callbacks.History` callbacks are created automatically\n",
      "            and need not be passed into `model.fit`.\n",
      "            `tf.keras.callbacks.ProgbarLogger` is created or not based on\n",
      "            `verbose` argument to `model.fit`.\n",
      "        validation_split: Float between 0 and 1.\n",
      "            Fraction of the training data to be used as validation data.\n",
      "            The model will set apart this fraction of the training data,\n",
      "            will not train on it, and will evaluate\n",
      "            the loss and any model metrics\n",
      "            on this data at the end of each epoch.\n",
      "            The validation data is selected from the last samples\n",
      "            in the `x` and `y` data provided, before shuffling. This argument is\n",
      "            not supported when `x` is a dataset, generator or\n",
      "           `keras.utils.Sequence` instance.\n",
      "        validation_data: Data on which to evaluate\n",
      "            the loss and any model metrics at the end of each epoch.\n",
      "            The model will not be trained on this data. Thus, note the fact\n",
      "            that the validation loss of data provided using `validation_split`\n",
      "            or `validation_data` is not affected by regularization layers like\n",
      "            noise and dropout.\n",
      "            `validation_data` will override `validation_split`.\n",
      "            `validation_data` could be:\n",
      "              - tuple `(x_val, y_val)` of Numpy arrays or tensors\n",
      "              - tuple `(x_val, y_val, val_sample_weights)` of Numpy arrays\n",
      "              - dataset\n",
      "            For the first two cases, `batch_size` must be provided.\n",
      "            For the last case, `validation_steps` could be provided.\n",
      "            Note that `validation_data` does not support all the data types that\n",
      "            are supported in `x`, eg, dict, generator or `keras.utils.Sequence`.\n",
      "        shuffle: Boolean (whether to shuffle the training data\n",
      "            before each epoch) or str (for 'batch'). This argument is ignored\n",
      "            when `x` is a generator. 'batch' is a special option for dealing\n",
      "            with the limitations of HDF5 data; it shuffles in batch-sized\n",
      "            chunks. Has no effect when `steps_per_epoch` is not `None`.\n",
      "        class_weight: Optional dictionary mapping class indices (integers)\n",
      "            to a weight (float) value, used for weighting the loss function\n",
      "            (during training only).\n",
      "            This can be useful to tell the model to\n",
      "            \"pay more attention\" to samples from\n",
      "            an under-represented class.\n",
      "        sample_weight: Optional Numpy array of weights for\n",
      "            the training samples, used for weighting the loss function\n",
      "            (during training only). You can either pass a flat (1D)\n",
      "            Numpy array with the same length as the input samples\n",
      "            (1:1 mapping between weights and samples),\n",
      "            or in the case of temporal data,\n",
      "            you can pass a 2D array with shape\n",
      "            `(samples, sequence_length)`,\n",
      "            to apply a different weight to every timestep of every sample. This\n",
      "            argument is not supported when `x` is a dataset, generator, or\n",
      "           `keras.utils.Sequence` instance, instead provide the sample_weights\n",
      "            as the third element of `x`.\n",
      "        initial_epoch: Integer.\n",
      "            Epoch at which to start training\n",
      "            (useful for resuming a previous training run).\n",
      "        steps_per_epoch: Integer or `None`.\n",
      "            Total number of steps (batches of samples)\n",
      "            before declaring one epoch finished and starting the\n",
      "            next epoch. When training with input tensors such as\n",
      "            TensorFlow data tensors, the default `None` is equal to\n",
      "            the number of samples in your dataset divided by\n",
      "            the batch size, or 1 if that cannot be determined. If x is a\n",
      "            `tf.data` dataset, and 'steps_per_epoch'\n",
      "            is None, the epoch will run until the input dataset is exhausted.\n",
      "            When passing an infinitely repeating dataset, you must specify the\n",
      "            `steps_per_epoch` argument. This argument is not supported with\n",
      "            array inputs.\n",
      "        validation_steps: Only relevant if `validation_data` is provided and\n",
      "            is a `tf.data` dataset. Total number of steps (batches of\n",
      "            samples) to draw before stopping when performing validation\n",
      "            at the end of every epoch. If 'validation_steps' is None, validation\n",
      "            will run until the `validation_data` dataset is exhausted. In the\n",
      "            case of an infinitely repeated dataset, it will run into an\n",
      "            infinite loop. If 'validation_steps' is specified and only part of\n",
      "            the dataset will be consumed, the evaluation will start from the\n",
      "            beginning of the dataset at each epoch. This ensures that the same\n",
      "            validation samples are used every time.\n",
      "        validation_batch_size: Integer or `None`.\n",
      "            Number of samples per validation batch.\n",
      "            If unspecified, will default to `batch_size`.\n",
      "            Do not specify the `validation_batch_size` if your data is in the\n",
      "            form of datasets, generators, or `keras.utils.Sequence` instances\n",
      "            (since they generate batches).\n",
      "        validation_freq: Only relevant if validation data is provided. Integer\n",
      "            or `collections_abc.Container` instance (e.g. list, tuple, etc.).\n",
      "            If an integer, specifies how many training epochs to run before a\n",
      "            new validation run is performed, e.g. `validation_freq=2` runs\n",
      "            validation every 2 epochs. If a Container, specifies the epochs on\n",
      "            which to run validation, e.g. `validation_freq=[1, 2, 10]` runs\n",
      "            validation at the end of the 1st, 2nd, and 10th epochs.\n",
      "        max_queue_size: Integer. Used for generator or `keras.utils.Sequence`\n",
      "            input only. Maximum size for the generator queue.\n",
      "            If unspecified, `max_queue_size` will default to 10.\n",
      "        workers: Integer. Used for generator or `keras.utils.Sequence` input\n",
      "            only. Maximum number of processes to spin up\n",
      "            when using process-based threading. If unspecified, `workers`\n",
      "            will default to 1. If 0, will execute the generator on the main\n",
      "            thread.\n",
      "        use_multiprocessing: Boolean. Used for generator or\n",
      "            `keras.utils.Sequence` input only. If `True`, use process-based\n",
      "            threading. If unspecified, `use_multiprocessing` will default to\n",
      "            `False`. Note that because this implementation relies on\n",
      "            multiprocessing, you should not pass non-picklable arguments to\n",
      "            the generator as they can't be passed easily to children processes.\n",
      "    \n",
      "    Unpacking behavior for iterator-like inputs:\n",
      "        A common pattern is to pass a tf.data.Dataset, generator, or\n",
      "      tf.keras.utils.Sequence to the `x` argument of fit, which will in fact\n",
      "      yield not only features (x) but optionally targets (y) and sample weights.\n",
      "      Keras requires that the output of such iterator-likes be unambiguous. The\n",
      "      iterator should return a tuple of length 1, 2, or 3, where the optional\n",
      "      second and third elements will be used for y and sample_weight\n",
      "      respectively. Any other type provided will be wrapped in a length one\n",
      "      tuple, effectively treating everything as 'x'. When yielding dicts, they\n",
      "      should still adhere to the top-level tuple structure.\n",
      "      e.g. `({\"x0\": x0, \"x1\": x1}, y)`. Keras will not attempt to separate\n",
      "      features, targets, and weights from the keys of a single dict.\n",
      "        A notable unsupported data type is the namedtuple. The reason is that\n",
      "      it behaves like both an ordered datatype (tuple) and a mapping\n",
      "      datatype (dict). So given a namedtuple of the form:\n",
      "          `namedtuple(\"example_tuple\", [\"y\", \"x\"])`\n",
      "      it is ambiguous whether to reverse the order of the elements when\n",
      "      interpreting the value. Even worse is a tuple of the form:\n",
      "          `namedtuple(\"other_tuple\", [\"x\", \"y\", \"z\"])`\n",
      "      where it is unclear if the tuple was intended to be unpacked into x, y,\n",
      "      and sample_weight or passed through as a single element to `x`. As a\n",
      "      result the data processing code will simply raise a ValueError if it\n",
      "      encounters a namedtuple. (Along with instructions to remedy the issue.)\n",
      "    \n",
      "    Returns:\n",
      "        A `History` object. Its `History.history` attribute is\n",
      "        a record of training loss values and metrics values\n",
      "        at successive epochs, as well as validation loss values\n",
      "        and validation metrics values (if applicable).\n",
      "    \n",
      "    Raises:\n",
      "        RuntimeError: 1. If the model was never compiled or,\n",
      "        2. If `model.fit` is  wrapped in `tf.function`.\n",
      "    \n",
      "        ValueError: In case of mismatch between the provided input data\n",
      "            and what the model expects or when the input data is empty.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(seq2seq_LSTM_model.fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86181021",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# add some callbacks before beginning training.\n",
    "checkpoint_cb = keras.callbacks.ModelCheckpoint(\"seq2seq_LSTM_model.h5\")\n",
    "\n",
    "seq2seq_LSTM_model.fit(train_generator(),\n",
    "                       #steps_per_epoch=60_000,\n",
    "                       epochs=1,\n",
    "                       validation_split=0.3,\n",
    "                       verbose=True,\n",
    "                       callbacks=[checkpoint_cb],\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "center-summary",
   "metadata": {},
   "source": [
    "Epoch 1/3\n",
    "  76176/Unknown - 847s 11ms/step - loss: 3.6019e-04"
   ]
  },
  {
   "cell_type": "raw",
   "id": "spatial-shipping",
   "metadata": {},
   "source": [
    "Epoch 1/2\n",
    "60000/60000 [==============================] - 244s 4ms/step - loss: 0.0873\n",
    "Epoch 2/2\n",
    "  480/60000 [..............................] - ETA: 4:02 - loss: 0.0096"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "framed-soundtrack",
   "metadata": {},
   "source": [
    "100 batches/sec \\imples 1 mil batches require 10^4 sec, i.e. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "governmental-ballot",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "166"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "10**4 // (60)  # minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "skilled-advocate",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = seq2seq_LSTM_model.predict(np.array([0.,9,8,3]).reshape((1,4,1)), batch_size=1)\n",
    "np.argmax(output[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mathematical-setting",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
